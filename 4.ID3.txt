import pandas as pd
import math
from pprint import pprint

def entropy(labels):
    total_samples = len(labels)
    unique_labels = set(labels)
    entropy_value = sum(-((labels.tolist().count(label) / total_samples) * math.log2(labels.tolist().count(label) / total_samples)) for label in unique_labels)
    return entropy_value

def information_gain(data, feature):
    total_entropy = entropy(data['PlayTennis'])
    weighted_entropy = sum((len(subset) / len(data)) * entropy(subset['PlayTennis']) for _, subset in data.groupby(feature))
    return total_entropy - weighted_entropy

def build_tree(data, features):
    labels = data['PlayTennis']

    if len(set(labels)) == 1:
        return labels.iloc[0]

    best_feature = max(features, key=lambda f: information_gain(data, f))
    tree = {best_feature: {}}
    
    for value, subset in data.groupby(best_feature):
        tree[best_feature][value] = build_tree(subset.drop(columns=[best_feature]), [f for f in features if f != best_feature])
    return tree

file_path = 'D:\ID3.csv'  
data = pd.read_csv(file_path)
decision_tree = build_tree(data, list(data.columns[:-1]))
pprint(decision_tree)
